{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4c3399-c0dc-417f-b4f0-02f51559bcfb",
   "metadata": {},
   "source": [
    "# BiasBusterAI: Preprocessing StereoSet Data for Bias Analysis\n",
    "\n",
    "This cell defines the `get_data` function for the **BiasBusterAI** project, which fetches and processes JSON data from the StereoSet dataset to prepare it for bias analysis. The function:\n",
    "\n",
    "1. Retrieves JSON data from a specified URL.\n",
    "2. Concatenates and flattens nested data structures.\n",
    "3. Explodes and normalizes sentence data.\n",
    "4. Combines context and sentence text, creates numeric bias labels, and formats the output.\n",
    "5. Returns a DataFrame (`text`, `bias_type`, `bias_label`) with lowercase text and a dictionary mapping bias types to numeric labels.\n",
    "\n",
    "The StereoSet dataset is used to evaluate biases in language models, aligning with **BiasBusterAI**'s goal of detecting and analyzing biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af6574-1b31-4399-90d1-f5c9ac7b0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def get_data(url: str) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Fetches and processes JSON data from a URL for bias analysis in BiasBusterAI.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL pointing to the JSON data (e.g., StereoSet dataset).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict[str, int]]: A tuple containing:\n",
    "            - A DataFrame with columns 'text' (lowercase, concatenated context and sentence),\n",
    "              'bias_type' (type of bias), and 'bias_label' (numeric mapping of bias types).\n",
    "            - A dictionary mapping bias types (str) to numeric labels (int).\n",
    "    \"\"\"\n",
    "    # Concatenate nested data from the first two rows of the second column\n",
    "    df_concated = pd.concat([pd.DataFrame(df.iloc[0,1]), pd.DataFrame(df.iloc[1,1])])\n",
    "    \n",
    "    # Explode the 'sentences' column to create a row for each sentence\n",
    "    df_exp = df_concated.explode(\"sentences\").reset_index(drop=True)\n",
    "    \n",
    "    # Normalize the 'sentences' column to flatten nested JSON structures\n",
    "    sentences_df = pd.json_normalize(df_exp[\"sentences\"])\n",
    "    \n",
    "    # Combine the exploded DataFrame (without 'sentences') with normalized sentences\n",
    "    df_flat = pd.concat([df_exp.drop(columns=[\"sentences\"]), sentences_df], axis=1)\n",
    "    \n",
    "    # Create a 'text' column by concatenating 'context' and 'sentence' with a space\n",
    "    df_flat[\"text\"] = (\n",
    "        df_flat[\"context\"].astype(str).str.strip() + \" \" +\n",
    "        df_flat[\"sentence\"].astype(str).str.strip()\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of unique bias types to numeric labels\n",
    "    bias_map = {b: i for i, b in enumerate(df_flat[\"bias_type\"].unique())}\n",
    "    \n",
    "    # Map bias types to numeric labels\n",
    "    df_flat[\"bias_label\"] = df_flat[\"bias_type\"].map(bias_map)\n",
    "    \n",
    "    # Select relevant columns and convert text to lowercase\n",
    "    final_df = df_flat[['text', 'bias_type', 'bias_label']]\n",
    "    final_df['text'] = final_df['text'].str.lower()\n",
    "    \n",
    "    return final_df, bias_map\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/moinnadeem/StereoSet/master/data/dev.json\"\n",
    "df, class_to_idx = get_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddeddfa-4aaa-4c10-a399-ddbe5e84489d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c627b5e8-0f79-4d8f-922f-43fc37b502f4",
   "metadata": {},
   "source": [
    "# BiasBusterAI: Splitting Data into Training and Validation Sets\n",
    "\n",
    "This cell defines the `get_train_val_dataset` function for the **BiasBusterAI** project, which splits a preprocessed DataFrame into training and validation sets for bias analysis. The function:\n",
    "\n",
    "1. Takes a DataFrame.\n",
    "2. Splits the data into training (80%) and validation (20%) sets, stratified by `bias_label` to maintain class distribution.\n",
    "3. Uses a fixed random seed for reproducibility.\n",
    "4. Returns the training and validation DataFrames.\n",
    "\n",
    "This function prepares the StereoSet dataset for training machine learning models to detect biases, aligning with **BiasBusterAI**'s objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7769a1-c2e8-4e93-ae44-f6947f8580ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def get_train_val_dataset(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Splits a DataFrame into training and validation sets for bias analysis in BiasBusterAI.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns including 'bias_label'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
    "            - The training DataFrame (80% of the data).\n",
    "            - The validation DataFrame (20% of the data).\n",
    "    \"\"\"\n",
    "    # Split the data into training (80%) and validation (20%) sets, stratified by bias_label\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, test_size=0.2, random_state=42, stratify=df[\"bias_label\"]\n",
    "    )\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = get_train_val_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386de46-fc21-4102-8b78-a737b66d6cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e4e6e4f-3409-47bb-949c-6d2be674e65d",
   "metadata": {},
   "source": [
    "# BiasBusterAI: Tokenizing and Preparing Text Data for Model Training\n",
    "\n",
    "This cell defines the `tokenize` function for the **BiasBusterAI** project, which processes text data for bias analysis. The function:\n",
    "\n",
    "1. Tokenizes text from training and validation DataFrames.\n",
    "2. Converts text to padded sequences for consistent input length.\n",
    "3. Creates TensorFlow datasets with batched data for model training.\n",
    "4. Returns the training and validation datasets along with the tokenizer.\n",
    "\n",
    "This function prepares the StereoSet dataset for training machine learning models to detect biases, aligning with **BiasBusterAI**'s objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908efbe0-6829-4672-bca9-571f1885766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from typing import Tuple\n",
    "\n",
    "def tokenize_and_prepare_datasets(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    max_len: int = 50,\n",
    "    batch_size: int = 32,\n",
    "    oov_token: str = \"<OOV>\"\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, Tokenizer]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Tokenizes and prepares text data from training and validation DataFrames for model training in BiasBusterAI.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): The training DataFrame with 'text' and 'bias_label' columns.\n",
    "        val_df (pd.DataFrame): The validation DataFrame with 'text' and 'bias_label' columns.\n",
    "        max_len (int): Maximum sequence length for padding (default: 50).\n",
    "        batch_size (int): Batch size for TensorFlow datasets (default: 32).\n",
    "        oov_token (str): Token for out-of-vocabulary words (default: \"<OOV>\").\n",
    "\n",
    "    Returns:\n",
    "        Tuple[tf.data.Dataset, tf.data.Dataset, Tokenizer]: A tuple containing:\n",
    "            - The training TensorFlow dataset (batched).\n",
    "            - The validation TensorFlow dataset (batched).\n",
    "            - The fitted Tokenizer object.\n",
    "    \"\"\"\n",
    "    # Initialize and fit tokenizer on training text\n",
    "    tokenizer = Tokenizer(oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(train_df[\"text\"])\n",
    "\n",
    "    # Convert text to sequences\n",
    "    X_train = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "    X_val = tokenizer.texts_to_sequences(val_df[\"text\"])\n",
    "\n",
    "    # Pad sequences to fixed length\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    X_val = pad_sequences(X_val, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    # Extract labels\n",
    "    y_train = train_df[\"bias_label\"].values\n",
    "    y_val = val_df[\"bias_label\"].values\n",
    "\n",
    "    # Create TensorFlow datasets with batching\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "    return train_ds, val_ds, tokenizer\n",
    "\n",
    "train_ds, val_ds, tokenizer = tokenize_and_prepare_datasets(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f01cf-f956-40a2-9fd5-6a77a80e0c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57a8f0d7-9b91-4efc-97f6-a0f37d56f496",
   "metadata": {},
   "source": [
    "# BiasBusterAI: Creating GloVe Embedding Matrix\n",
    "\n",
    "This cell defines the `create_embedding_matrix` function for the **BiasBusterAI** project, which generates an embedding matrix for tokenized text. The function:\n",
    "\n",
    "1. Takes a fitted tokenizer, vocabulary size, embedding dimension, and path to a GloVe embedding file.\n",
    "2. Loads pre-trained GloVe word vectors.\n",
    "3. Maps vocabulary words to their corresponding GloVe embeddings.\n",
    "4. Returns an embedding matrix for use in neural network models.\n",
    "\n",
    "This function enhances **BiasBusterAI**’s ability to leverage pre-trained embeddings for bias detection in the StereoSet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6b022-d583-4fe2-a6fe-62138f4fd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_embedding_matrix(\n",
    "    tokenizer: Tokenizer,\n",
    "    vocab_size: int,\n",
    "    embedding_dim: int,\n",
    "    glove_file: str\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Creates an embedding matrix using pre-trained GloVe embeddings for BiasBusterAI.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): The fitted Keras Tokenizer with word index.\n",
    "        vocab_size (int): The size of the vocabulary (including reserved indices).\n",
    "        embedding_dim (int): The dimension of the GloVe embeddings.\n",
    "        glove_file (str): Path to the GloVe embedding file (e.g., 'glove.6B.100d.txt').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An embedding matrix of shape (vocab_size, embedding_dim) with GloVe vectors.\n",
    "    \"\"\"\n",
    "    # Initialize embedding matrix with zeros\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    # Load GloVe embeddings from file\n",
    "    with open(glove_file, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            # Map word to its GloVe vector if it exists in the tokenizer's word index\n",
    "            if word in tokenizer.word_index and tokenizer.word_index[word] < vocab_size:\n",
    "                embedding_matrix[tokenizer.word_index[word]] = np.asarray(values[1:], dtype=\"float32\")\n",
    "                \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, vocab_size, embedding_dim, glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a19ad3-4bad-48a7-a3a3-182de4886fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa1d21eb-f896-494a-be6f-31c0d84f27e8",
   "metadata": {},
   "source": [
    "# BiasBusterAI: Building BiLSTM Attention Model for Bias Classification\n",
    "\n",
    "This cell defines the `build_bilstm_attention_model` function for the **BiasBusterAI** project, which constructs a neural network model for bias classification. The function:\n",
    "\n",
    "1. Takes an embedding matrix, vocabulary size, embedding dimension, maximum sequence length, and number of classes.\n",
    "2. Creates a model with a non-trainable embedding layer using pre-trained GloVe embeddings.\n",
    "3. Applies a bidirectional LSTM layer followed by a custom attention mechanism.\n",
    "4. Adds a dense output layer for bias classification.\n",
    "5. Returns a compiled TensorFlow model.\n",
    "\n",
    "This function supports **BiasBusterAI**’s goal of detecting biases in the StereoSet dataset using an advanced attention-based neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baec0d3-3fbf-4f66-b5b4-a6914cf36734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A custom Keras layer that implements an attention mechanism for sequence data in BiasBusterAI.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        # Create the Dense layer for attention scores\n",
    "        self.dense = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Computes attention weights and a context vector for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor of shape (batch, seq_len, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[tf.Tensor, tf.Tensor]: A tuple containing:\n",
    "                - Context vector of shape (batch, hidden_size).\n",
    "                - Attention weights of shape (batch, seq_len, 1).\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = self.dense(inputs)            # (batch, seq_len, 1)\n",
    "        # Normalize scores to obtain attention weights\n",
    "        weights = tf.nn.softmax(scores, axis=1) # (batch, seq_len, 1)\n",
    "        # Compute context vector as weighted sum of inputs\n",
    "        context_vector = tf.reduce_sum(weights * inputs, axis=1)  # (batch, hidden_size)\n",
    "        return context_vector, weights\n",
    "\n",
    "class BiLSTMAttentionModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A Keras model for bias classification using a bidirectional LSTM and attention mechanism in BiasBusterAI.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary for the embedding layer.\n",
    "        embedding_dim (int): Dimension of the embeddings.\n",
    "        embedding_matrix: Pre-trained embedding matrix of shape (vocab_size, embedding_dim).\n",
    "        max_len (int): Maximum sequence length for input text.\n",
    "        num_classes (int): Number of bias classes for classification.\n",
    "        lstm_units (int): Number of units in the LSTM layer (default: 128).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        embedding_matrix: np.ndarray,\n",
    "        max_len: int,\n",
    "        num_classes: int,\n",
    "        lstm_units: int = 128\n",
    "    ):\n",
    "        super(BiLSTMAttentionModel, self).__init__()\n",
    "        # Non-trainable embedding layer with pre-trained weights\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim=vocab_size + 1,\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            trainable=False\n",
    "        )\n",
    "        # Bidirectional LSTM to capture sequential dependencies\n",
    "        self.bilstm = layers.Bidirectional(\n",
    "            layers.LSTM(lstm_units, return_sequences=True)\n",
    "        )\n",
    "        # Custom attention layer\n",
    "        self.attention = AttentionLayer()\n",
    "        # Output layer for classification\n",
    "        self.fc = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Defines the forward pass of the model for bias classification.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor of shape (batch, seq_len).\n",
    "            training (bool): Whether the model is in training mode (default: False).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output probabilities of shape (batch, num_classes).\n",
    "        \"\"\"\n",
    "        # Apply embedding layer\n",
    "        x = self.embedding(inputs)               # (batch, seq_len, embed_dim)\n",
    "        # Apply bidirectional LSTM\n",
    "        x = self.bilstm(x)                      # (batch, seq_len, 2*lstm_units)\n",
    "        # Apply attention to get context vector\n",
    "        context, _ = self.attention(x)          # (batch, 2*lstm_units), (batch, seq_len, 1)\n",
    "        # Compute output probabilities\n",
    "        output = self.fc(context)               # (batch, num_classes)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20979861-b7b6-4d89-83a5-115083a763fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33646b6a-5899-4fd1-a582-a2bf6b2e6ae8",
   "metadata": {},
   "source": [
    "# BiasBusterAI: Building and Compiling BiLSTM Attention Model\n",
    "\n",
    "This cell defines the `build_and_compile_bilstm_model` function for the **BiasBusterAI** project, which constructs and compiles a neural network model for bias classification. The function:\n",
    "\n",
    "1. Takes an embedding matrix, vocabulary size, embedding dimension, maximum sequence length, and number of classes.\n",
    "2. Creates a BiLSTM model with a non-trainable embedding layer, bidirectional LSTM, attention mechanism, and dense output layer.\n",
    "3. Compiles the model with the Adam optimizer and sparse categorical crossentropy loss.\n",
    "4. Returns the compiled TensorFlow model.\n",
    "\n",
    "This function supports **BiasBusterAI**’s goal of detecting biases in the StereoSet dataset using an attention-based neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4469ed3-37c0-4d3c-a536-47643af72822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def build_and_compile_bilstm_model(\n",
    "    vocab_size: int,\n",
    "    embedding_dim: int,\n",
    "    embedding_matrix,\n",
    "    max_len: int,\n",
    "    num_classes: int,\n",
    "    lstm_units: int = 128\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Builds and compiles a BiLSTM model with attention for bias classification in BiasBusterAI.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary for the embedding layer.\n",
    "        embedding_dim (int): Dimension of the embeddings.\n",
    "        embedding_matrix: Pre-trained embedding matrix of shape (vocab_size, embedding_dim).\n",
    "        max_len (int): Maximum sequence length for input text.\n",
    "        num_classes (int): Number of bias classes for classification.\n",
    "        lstm_units (int): Number of units in the LSTM layer (default: 128).\n",
    "\n",
    "    Returns:\n",
    "        A compiled TensorFlow/Keras model for bias classification.\n",
    "    \"\"\"\n",
    "    # Instantiate the BiLSTMAttentionModel\n",
    "    model = BiLSTMAttentionModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        max_len=max_len,\n",
    "        num_classes=num_classes,\n",
    "        lstm_units=lstm_units\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "68c71193-dcfc-4d57-b953-3023d9299590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 17:04:35.420396: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [10149]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2025-10-05 17:04:35.764385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-10-05 17:04:35.766960: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-10-05 17:04:35.768770: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-10-05 17:04:36.077940: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2025-10-05 17:04:36.160033: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-10-05 17:04:36.162551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-10-05 17:04:36.164349: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-10-05 17:04:37.664706: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2025-10-05 17:04:38.941181: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-10-05 17:04:38.944343: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-10-05 17:04:38.946806: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-10-05 17:04:39.240047: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2025-10-05 17:04:39.408877: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-10-05 17:04:39.410927: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-10-05 17:04:39.412897: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-10-05 17:04:40.543745: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/318 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8831"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 17:05:48.413799: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [2538]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2025-10-05 17:05:49.102490: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-10-05 17:05:49.104654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-10-05 17:05:49.106385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-10-05 17:05:49.382331: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2025-10-05 17:05:49.481277: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-10-05 17:05:49.483368: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-10-05 17:05:49.485110: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/318 [==============================] - 82s 236ms/step - loss: 0.3454 - accuracy: 0.8831 - val_loss: 0.0759 - val_accuracy: 0.9791\n",
      "Epoch 2/10\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 0.0518 - accuracy: 0.9841 - val_loss: 0.0309 - val_accuracy: 0.9913\n",
      "Epoch 3/10\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 0.0203 - accuracy: 0.9950 - val_loss: 0.0173 - val_accuracy: 0.9941\n",
      "Epoch 4/10\n",
      "318/318 [==============================] - 63s 199ms/step - loss: 0.0092 - accuracy: 0.9976 - val_loss: 0.0081 - val_accuracy: 0.9968\n",
      "Epoch 5/10\n",
      "318/318 [==============================] - 66s 207ms/step - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.0085 - val_accuracy: 0.9965\n",
      "Epoch 6/10\n",
      "318/318 [==============================] - 66s 209ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0080 - val_accuracy: 0.9976\n",
      "Epoch 7/10\n",
      "318/318 [==============================] - 69s 218ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0048 - val_accuracy: 0.9988\n",
      "Epoch 8/10\n",
      "318/318 [==============================] - 69s 217ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0042 - val_accuracy: 0.9984\n",
      "Epoch 9/10\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "Epoch 10/10\n",
      "318/318 [==============================] - 71s 221ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0023 - val_accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,       \n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e9fd897d-44c2-4591-9474-db2085cf7728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8033, 8034)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X_train), vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c7999846-8a65-402b-8709-ca2898111017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bi_lstm_attention_model_6\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " embedding_6 (Embedding)     multiple                  803400    N          \n",
      "                                                                            \n",
      " bidirectional_6 (Bidirectio  multiple                 234496    Y          \n",
      " nal)                                                                       \n",
      "                                                                            \n",
      " attention_layer_6 (Attentio  multiple                 257       Y          \n",
      " nLayer)                                                                    \n",
      "                                                                            \n",
      " dense_12 (Dense)            multiple                  1028      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 1,039,181\n",
      "Trainable params: 235,781\n",
      "Non-trainable params: 803,400\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5cf9a37a-dec9-4625-931f-143551fda254",
   "metadata": {},
   "outputs": [],
   "source": [
    "context, attn_weights = model.attention(model.bilstm(model.embedding(sample_input)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75771ed-416e-44fb-9992-caec1cb9d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "weights = tf.squeeze(attn_weights)[0]\n",
    "tokens = tokenizer.sequences_to_texts([X_input[0]])[0].split()\n",
    "valid_len = np.count_nonzero(X_input[0])\n",
    "tokens = tokens[:valid_len]\n",
    "weights = weights[:valid_len]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 1))\n",
    "sns.heatmap([weights], annot=True, cmap='Blues', xticklabels=tokens, yticklabels=[], cbar=True)\n",
    "plt.title(\"Attention weights for the sentence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4050b-175c-427a-8cba-1ed60a8726b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
